import { ChunkerService } from './index'

async function testRealDocument() {
  console.log('ðŸ“„ REAL DOCUMENT CHUNKING TEST\n')
  
  const chunkerService = new ChunkerService()
  
  // Test vá»›i document size khÃ¡c nhau
  const documents = {
    small: `
# Introduction

This is a small document to test basic chunking functionality. It contains a few paragraphs and sections.

## Main Content

The chunking system should handle this document as a single chunk since it's quite short.

## Conclusion

This concludes our small test document.
    `.trim(),
    
    medium: `
# Technical Documentation

This document explains the architecture of our translation system.

## Overview

Our translation platform uses advanced AI models to provide high-quality translations across multiple languages. The system is designed with scalability and accuracy in mind.

### Key Features

1. **Multi-language Support**: We support over 50 languages with native-level accuracy.
2. **Context Preservation**: Our chunking algorithm maintains context across segments.
3. **Format Retention**: Documents maintain their original formatting.

## Architecture

### Core Components

The system consists of several key components:

#### Parser Service
Handles document parsing for various formats including PDF, DOCX, and plain text. The parser extracts content while preserving structure.

#### Chunking Engine
Our smart chunking engine divides documents into optimal segments for translation. It considers:
- Semantic boundaries
- Token limits
- Context preservation
- Language-specific requirements

#### Translation Pipeline
The translation pipeline processes chunks through multiple stages:
1. Pre-processing and normalization
2. Translation using AI models
3. Post-processing and quality checks
4. Format reconstruction

### Implementation Details

\`\`\`javascript
class TranslationService {
  async translate(document, options) {
    const chunks = await this.chunker.chunk(document);
    const translations = await Promise.all(
      chunks.map(chunk => this.translateChunk(chunk))
    );
    return this.assembleDocument(translations);
  }
}
\`\`\`

## Performance Considerations

The system is optimized for both speed and quality. Key optimizations include:
- Parallel chunk processing
- Intelligent caching
- Adaptive chunk sizing
- Resource pooling

## Conclusion

This architecture provides a robust foundation for high-quality document translation.
    `.trim(),
    
    vietnamese: `
# HÆ°á»›ng dáº«n sá»­ dá»¥ng há»‡ thá»‘ng

TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n cÃ¡ch sá»­ dá»¥ng há»‡ thá»‘ng dá»‹ch thuáº­t AI.

## Giá»›i thiá»‡u

Há»‡ thá»‘ng dá»‹ch thuáº­t cá»§a chÃºng tÃ´i sá»­ dá»¥ng cÃ´ng nghá»‡ AI tiÃªn tiáº¿n Ä‘á»ƒ cung cáº¥p báº£n dá»‹ch cháº¥t lÆ°á»£ng cao. Há»‡ thá»‘ng há»— trá»£ nhiá»u ngÃ´n ngá»¯ vÃ  Ä‘á»‹nh dáº¡ng tÃ i liá»‡u khÃ¡c nhau.

### TÃ­nh nÄƒng chÃ­nh

- Dá»‹ch Ä‘a ngÃ´n ngá»¯ vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao
- Giá»¯ nguyÃªn format vÃ  cáº¥u trÃºc tÃ i liá»‡u
- Xá»­ lÃ½ nhanh vá»›i cÃ´ng nghá»‡ chunking thÃ´ng minh
- Há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng file

## CÃ¡ch sá»­ dá»¥ng

### BÆ°á»›c 1: Táº£i lÃªn tÃ i liá»‡u

NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ táº£i lÃªn tÃ i liá»‡u báº±ng cÃ¡ch:
1. KÃ©o tháº£ file vÃ o khu vá»±c upload
2. Click vÃ o nÃºt táº£i lÃªn vÃ  chá»n file
3. Paste ná»™i dung trá»±c tiáº¿p

### BÆ°á»›c 2: Chá»n ngÃ´n ngá»¯ Ä‘Ã­ch

Há»‡ thá»‘ng tá»± Ä‘á»™ng nháº­n diá»‡n ngÃ´n ngá»¯ nguá»“n. Báº¡n chá»‰ cáº§n chá»n ngÃ´n ngá»¯ muá»‘n dá»‹ch sang.

### BÆ°á»›c 3: Xá»­ lÃ½ vÃ  táº£i xuá»‘ng

Sau khi dá»‹ch xong, báº¡n cÃ³ thá»ƒ:
- Xem trÆ°á»›c káº¿t quáº£
- Táº£i xuá»‘ng file Ä‘Ã£ dá»‹ch
- Copy ná»™i dung

## LÆ°u Ã½ quan trá»ng

- File khÃ´ng Ä‘Æ°á»£c vÆ°á»£t quÃ¡ 10MB
- Äá»‹nh dáº¡ng phá»©c táº¡p cÃ³ thá»ƒ máº¥t thá»i gian xá»­ lÃ½ lÃ¢u hÆ¡n
- NÃªn kiá»ƒm tra láº¡i káº¿t quáº£ vá»›i tÃ i liá»‡u chuyÃªn ngÃ nh
    `.trim()
  }
  
  // Test each document
  for (const [type, content] of Object.entries(documents)) {
    console.log(`\n${'='.repeat(60)}`)
    console.log(`Testing ${type.toUpperCase()} document (${content.length} chars)`)
    console.log('='.repeat(60))
    
    // Analyze first
    const analysis = await chunkerService.analyzeDocument(content)
    console.log('\nðŸ“Š Analysis:')
    console.log(`- Estimated tokens: ${analysis.estimatedTokens}`)
    console.log(`- Recommended tier: ${analysis.recommendedTier}`)
    console.log(`- Estimated chunks: B:${analysis.estimatedChunks.basic} / S:${analysis.estimatedChunks.standard} / P:${analysis.estimatedChunks.premium}`)
    
    // Test recommended tier
    const tier = analysis.recommendedTier
    console.log(`\nðŸ”§ Chunking with ${tier} tier...`)
    
    const result = await chunkerService.chunkDocument(content, tier, {
      generateDNA: true // Enable DNA for testing
    })
    
    console.log(`\nâœ… Results:`)
    console.log(`- Chunks created: ${result.chunks.length}`)
    console.log(`- Total tokens: ${result.totalTokens}`)
    console.log(`- Processing time: ${result.metadata.processingTime}ms`)
    
    // Show chunk details
    if (result.chunks.length > 0) {
      console.log('\nðŸ“¦ Chunk Details:')
      result.chunks.forEach((chunk, idx) => {
        console.log(`\nChunk ${idx + 1}:`)
        console.log(`- Tokens: ${chunk.tokens}`)
        console.log(`- Length: ${chunk.content.length} chars`)
        
        if (chunk.metadata?.dna) {
          const dna = chunk.metadata.dna
          console.log(`- Language: ${dna.language}`)
          console.log(`- Style: ${dna.style}`)
          console.log(`- Complexity: ${dna.complexity}/10`)
          if (dna.hasCode) console.log('- Contains code: Yes')
          if (dna.topics.length > 0) {
            console.log(`- Topics: ${dna.topics.slice(0, 3).join(', ')}`)
          }
        }
        
        console.log(`- Preview: "${chunk.content.substring(0, 60)}..."`)
      })
    }
  }
  
  // Test error handling
  console.log(`\n\n${'='.repeat(60)}`)
  console.log('ERROR HANDLING TEST')
  console.log('='.repeat(60))
  
  try {
    await chunkerService.chunkDocument('', 'standard')
  } catch (error) {
    console.log('âœ… Empty text handled correctly:', error.message)
  }
  
  // Performance test
  console.log(`\n\n${'='.repeat(60)}`)
  console.log('PERFORMANCE TEST')
  console.log('='.repeat(60))
  
  const largeDoc = documents.medium.repeat(10) // ~10x medium doc
  console.log(`\nTesting large document (${largeDoc.length} chars)...`)
  
  const startTime = Date.now()
  const perfResult = await chunkerService.chunkDocument(largeDoc, 'standard')
  const totalTime = Date.now() - startTime
  
  console.log(`âœ… Performance Results:`)
  console.log(`- Document size: ${largeDoc.length} chars`)
  console.log(`- Chunks created: ${perfResult.chunks.length}`)
  console.log(`- Total tokens: ${perfResult.totalTokens}`)
  console.log(`- Total time: ${totalTime}ms`)
  console.log(`- Throughput: ${Math.round(largeDoc.length / totalTime * 1000)} chars/sec`)
  
  console.log('\nðŸŽ‰ All tests completed successfully!')
}

testRealDocument().catch(console.error)