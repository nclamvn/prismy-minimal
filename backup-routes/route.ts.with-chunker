import { NextResponse } from 'next/server'
import OpenAI from 'openai'
import { ChunkerService } from '@/lib/services/chunker/chunker.service'
import type { ChunkingTier } from '@/lib/services/chunker/chunker.service'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
})

const chunkerService = new ChunkerService()

const LANG_MAP = {
  vi: 'Vietnamese',
  en: 'English',
  zh: 'Chinese (Simplified)',
  ja: 'Japanese',
  ko: 'Korean',
  fr: 'French',
  es: 'Spanish',
  de: 'German',
}

// Tier configuration
const TIER_CONFIG = {
  basic: {
    model: 'gpt-3.5-turbo',
    maxChars: 50000,
    chunkingTier: 'basic' as ChunkingTier,
    temperature: 0.3,
    batchSize: 5,
    delay: 200,
  },
  standard: {
    model: 'gpt-4o',
    maxChars: 200000,
    chunkingTier: 'standard' as ChunkingTier,
    temperature: 0.4,
    batchSize: 3,
    delay: 500,
  },
  premium: {
    model: 'gpt-4o',
    maxChars: 1000000,
    chunkingTier: 'premium' as ChunkingTier,
    temperature: 0.4,
    batchSize: 2,
    delay: 8000,
  },
}

export async function GET() {
  return NextResponse.json({ 
    message: "Prismy Translation API v2.0",
    hasApiKey: !!process.env.OPENAI_API_KEY,
    supportedLanguages: Object.keys(LANG_MAP),
    tiers: Object.keys(TIER_CONFIG),
    features: [
      'Smart chunking with GPT-3 encoder',
      'DNA extraction for document analysis',
      'Context preservation across chunks',
      'Multi-tier quality levels',
      'Rate limit protection'
    ]
  })
}

export async function POST(request: Request) {
  const startTime = Date.now()
  
  try {
    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json(
        { success: false, error: 'Missing OpenAI API key' },
        { status: 500 }
      )
    }

    const body = await request.json()
    const { 
      text = '', 
      targetLang = 'vi', 
      tier = 'standard',
      isRewrite = false,
      generateDNA = true,
      preserveFormatting = true
    } = body
    
    // Validation
    if (!text.trim()) {
      return NextResponse.json(
        { success: false, error: 'No text provided' },
        { status: 400 }
      )
    }
    
    const config = TIER_CONFIG[tier as keyof typeof TIER_CONFIG] || TIER_CONFIG.standard
    
    if (text.length > config.maxChars) {
      return NextResponse.json(
        { success: false, error: `Text too long for ${tier} tier. Maximum ${config.maxChars} characters.` },
        { status: 400 }
      )
    }
    
    const targetLanguage = LANG_MAP[targetLang as keyof typeof LANG_MAP] || 'Vietnamese'
    
    console.log('=== Translation Request ===')
    console.log(`Text length: ${text.length} chars`)
    console.log(`Target: ${targetLanguage}`)
    console.log(`Tier: ${tier}`)
    console.log(`Model: ${config.model}`)
    
    // Chunking vá»›i ChunkerService
    const chunkResult = await chunkerService.chunkDocument(text, config.chunkingTier, {
      generateDNA,
      preserveStructure: preserveFormatting,
      maxTokens: tier === 'basic' ? 500 : tier === 'standard' ? 1000 : 1500,
      overlap: tier === 'premium' ? 200 : 100,
    })
    
    console.log(`=== Chunking Complete ===`)
    console.log(`Chunks created: ${chunkResult.chunks.length}`)
    console.log(`Total tokens: ${chunkResult.summary.totalTokens}`)
    
    if (chunkResult.chunks.length === 0) {
      throw new Error('No chunks created')
    }
    
    // Build system prompt based on tier and DNA
    const systemPrompt = buildSystemPrompt(tier, targetLanguage, isRewrite, chunkResult.chunks[0]?.metadata?.dna)
    
    // Translate chunks
    const translations: string[] = []
    let tokensUsed = 0
    let requestCount = 0
    
    // Process in batches
    for (let i = 0; i < chunkResult.chunks.length; i += config.batchSize) {
      const batch = chunkResult.chunks.slice(i, i + config.batchSize)
      console.log(`Processing batch ${Math.floor(i / config.batchSize) + 1}/${Math.ceil(chunkResult.chunks.length / config.batchSize)}`)
      
      const batchPromises = batch.map(async (chunk, batchIndex) => {
        const chunkIndex = i + batchIndex
        const previousContext = chunkIndex > 0 ? translations[chunkIndex - 1]?.slice(-200) : undefined
        
        try {
          return await translateChunk(
            chunk.content,
            systemPrompt,
            config.model,
            config.temperature,
            chunk.metadata?.dna,
            previousContext,
            chunkIndex,
            chunkResult.chunks.length
          )
        } catch (error: any) {
          if (error.status === 429 && config.model === 'gpt-4o') {
            console.log('Rate limit hit, falling back to GPT-3.5-turbo')
            return await translateChunk(
              chunk.content,
              systemPrompt,
              'gpt-3.5-turbo',
              config.temperature,
              chunk.metadata?.dna,
              previousContext,
              chunkIndex,
              chunkResult.chunks.length
            )
          }
          throw error
        }
      })
      
      const batchResults = await Promise.all(batchPromises)
      translations.push(...batchResults)
      
      // Track usage
      tokensUsed += batch.reduce((sum, chunk) => sum + chunk.tokens, 0)
      requestCount += batch.length
      
      // Rate limit protection
      if (i + config.batchSize < chunkResult.chunks.length) {
        const dynamicDelay = calculateDynamicDelay(config.delay, i, chunkResult.chunks.length, tokensUsed)
        await new Promise(resolve => setTimeout(resolve, dynamicDelay))
      }
    }
    
    // Join translations
    const finalTranslation = translations.join('\n\n')
    const processingTime = Date.now() - startTime
    
    return NextResponse.json({
      success: true,
      original: text,
      translated: finalTranslation,
      targetLang,
      model: config.model,
      tier,
      method: chunkResult.chunks.length > 1 ? 'chunked' : 'simple',
      stats: {
        originalLength: text.length,
        translatedLength: finalTranslation.length,
        processingTime,
        chunks: chunkResult.chunks.length,
        tokensProcessed: tokensUsed,
        requestCount,
        chunkingStrategy: chunkResult.summary.strategy,
        dna: generateDNA ? chunkResult.chunks[0]?.metadata?.dna : undefined
      }
    })
    
  } catch (error: any) {
    console.error('Translation error:', error.message)
    
    if (error.status === 401) {
      return NextResponse.json(
        { success: false, error: 'Invalid API key' },
        { status: 401 }
      )
    }
    
    if (error.status === 429) {
      return NextResponse.json(
        { success: false, error: 'Rate limit exceeded. Please try again later.' },
        { status: 429 }
      )
    }
    
    if (error.code === 'context_length_exceeded') {
      return NextResponse.json(
        { success: false, error: 'Text too long for model. Try using a lower tier or shorter text.' },
        { status: 400 }
      )
    }
    
    return NextResponse.json({ 
      success: false,
      error: error.message || 'Translation failed' 
    }, { status: 500 })
  }
}

function buildSystemPrompt(
  tier: string, 
  targetLanguage: string, 
  isRewrite: boolean,
  dna?: any
): string {
  if (isRewrite) {
    return `Rewrite this ${targetLanguage} text to be clearer and more natural. Maintain the original meaning and tone. Return only the rewritten text.`
  }
  
  let basePrompt = `Translate to ${targetLanguage}.`
  
  // Add DNA-based instructions
  if (dna) {
    if (dna.style === 'technical') {
      basePrompt += ' Maintain technical terminology and precision.'
    } else if (dna.style === 'academic') {
      basePrompt += ' Preserve academic tone and formal language.'
    } else if (dna.style === 'narrative') {
      basePrompt += ' Keep the narrative flow and storytelling elements.'
    }
    
    if (dna.hasCode) {
      basePrompt += ' Keep code blocks unchanged.'
    }
    
    if (dna.hasTable) {
      basePrompt += ' Preserve table formatting.'
    }
  }
  
  // Tier-specific instructions
  switch (tier) {
    case 'basic':
      basePrompt += ' Return only the translation.'
      break
    case 'standard':
      basePrompt += ' Maintain the original tone and style. Ensure natural flow.'
      break
    case 'premium':
      basePrompt += ' You are a professional translator. Preserve all nuances, cultural context, and ensure the translation reads naturally to native speakers. Maintain consistency in terminology throughout.'
      break
  }
  
  return basePrompt
}

async function translateChunk(
  content: string,
  systemPrompt: string,
  model: string,
  temperature: number,
  dna?: any,
  previousContext?: string,
  chunkIndex?: number,
  totalChunks?: number
): Promise<string> {
  let userPrompt = content
  
  // Add context for continuity
  if (previousContext && chunkIndex && chunkIndex > 0) {
    userPrompt = `[Previous context: ...${previousContext}]\n\n${content}`
  }
  
  // Add chunk info for premium tier
  if (model === 'gpt-4o' && totalChunks && totalChunks > 1) {
    userPrompt += `\n\n[This is part ${chunkIndex! + 1} of ${totalChunks}]`
  }
  
  const completion = await openai.chat.completions.create({
    model,
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt }
    ],
    temperature,
    max_tokens: Math.min(content.length * 3, 4000),
  })
  
  return completion.choices[0]?.message?.content || ''
}

function calculateDynamicDelay(
  baseDelay: number, 
  currentIndex: number, 
  totalChunks: number,
  tokensUsed: number
): number {
  // Increase delay as we process more chunks
  const progressFactor = currentIndex / totalChunks
  const tokenFactor = Math.min(tokensUsed / 50000, 2) // Cap at 2x
  
  return Math.floor(baseDelay * (1 + progressFactor) * (1 + tokenFactor))
}